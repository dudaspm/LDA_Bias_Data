{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw - an Implementation of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further understand the importance of topic modeling, we will be looking into a public dataset, i.e. the Jigsaw Unintended Bias dataset {cite}`kaggle`. \n",
    "\n",
    "\n",
    "This dataset consists of ~2m public comments from the Civil Comment {cite}`bogdanoff_2017` platform so that researchers could understand and improve civility in online conversations. Civil Comments was a social media platform that uses peer-review submission where commenters rated the civility of other comments before their own was, in turn, rated by others.\n",
    "\n",
    "These comments are then annotated by human raters for various toxic conversational attributes. Additional labels related to sociodemographic identifiers were mentioned to help a machine understand bias better, based on context analysis. But we will see the utility of this dataset and the presence of potential bias in these conversations related to people with physical, mental and learning disabilites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have filtered all the comments that have been provided a value for the parameters of 'Intellectual or Learning Disability'. 'Psychiatric or Mental Illness'. 'Physical Disability' & 'Other Disability'. We now have 18665 statements in this corpus. \n",
    "The dataset therefore contains statements pertained to these parameters. We can assume that these statements might most likely take about people with disability, but with the help of topic modeling, we can confirm this. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Some comments in the dataset may have explicit language.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the necessary libraries here. One library will also need to be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be reading the data into a dataframe for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'https://raw.githubusercontent.com/dudaspm/LDA_Bias_Data/main/PWD.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7705</td>\n",
       "      <td>6216834</td>\n",
       "      <td>No sympathy for these two knuckleheads.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-10-25 00:52:00.913992+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>392998</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8073</td>\n",
       "      <td>5625069</td>\n",
       "      <td>Wow!\\nYour progressive psychosis has become ex...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-07-20 03:30:15.579733+00</td>\n",
       "      <td>54</td>\n",
       "      <td>5624305.0</td>\n",
       "      <td>357183</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8115</td>\n",
       "      <td>5690713</td>\n",
       "      <td>Or.... maybe there IS chaos because the \"presi...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-07-31 17:02:58.167475+00</td>\n",
       "      <td>102</td>\n",
       "      <td>5690153.0</td>\n",
       "      <td>361265</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8125</td>\n",
       "      <td>470493</td>\n",
       "      <td>I'll take someone who's physically ill over on...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-09-12 02:41:50.084427+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145747</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8263</td>\n",
       "      <td>941207</td>\n",
       "      <td>Mental Illness at work again, again, again, ag...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-02-02 22:38:09.291374+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165832</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                                       comment_text  \\\n",
       "0        7705  6216834            No sympathy for these two knuckleheads.   \n",
       "1        8073  5625069  Wow!\\nYour progressive psychosis has become ex...   \n",
       "2        8115  5690713  Or.... maybe there IS chaos because the \"presi...   \n",
       "3        8125   470493  I'll take someone who's physically ill over on...   \n",
       "4        8263   941207  Mental Illness at work again, again, again, ag...   \n",
       "\n",
       "   split                   created_date  publication_id  parent_id  \\\n",
       "0  train  2017-10-25 00:52:00.913992+00              21        NaN   \n",
       "1  train  2017-07-20 03:30:15.579733+00              54  5624305.0   \n",
       "2  train  2017-07-31 17:02:58.167475+00             102  5690153.0   \n",
       "3  train  2016-09-12 02:41:50.084427+00              21        NaN   \n",
       "4  train  2017-02-02 22:38:09.291374+00              13        NaN   \n",
       "\n",
       "   article_id    rating  funny  ...  white  asian  latino  \\\n",
       "0      392998  approved      1  ...    0.0    0.0     0.0   \n",
       "1      357183  rejected      0  ...    0.0    0.0     0.0   \n",
       "2      361265  approved      0  ...    0.0    0.0     0.0   \n",
       "3      145747  approved      0  ...    0.0    0.0     0.0   \n",
       "4      165832  rejected      0  ...    0.0    0.0     0.0   \n",
       "\n",
       "   other_race_or_ethnicity  physical_disability  \\\n",
       "0                      0.0                 0.25   \n",
       "1                      0.0                 0.00   \n",
       "2                      0.0                 0.00   \n",
       "3                      0.0                 0.75   \n",
       "4                      0.0                 0.00   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
       "0                                  0.0                            0.0   \n",
       "1                                  0.0                            1.0   \n",
       "2                                  0.0                            1.0   \n",
       "3                                  0.0                            1.0   \n",
       "4                                  0.0                            1.0   \n",
       "\n",
       "   other_disability  identity_annotator_count  toxicity_annotator_count  \n",
       "0               0.0                         4                        58  \n",
       "1               0.0                         4                        10  \n",
       "2               0.0                         4                        62  \n",
       "3               0.0                         4                        68  \n",
       "4               0.0                         4                        70  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are just a few of the comments in this group. You can view more comments by changing the parameter in the 'head' function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                          No sympathy for these two knuckleheads.\n",
       "1                                        Wow!\\nYour progressive psychosis has become extreme!\\nPlease seek immediate medical help.\n",
       "2    Or.... maybe there IS chaos because the \"president\" is a mentally ill, in-over-his-head idiot who couldn't lead cats to tuna.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.comment_text.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets try removing unnecessary words and cleaning the statements for analysis of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "\n",
    "# cleaning master function\n",
    "def clean_tweet(tweet, bigrams=False):\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    tweet_token_list = [word for word in tweet.split(' ')\n",
    "                            if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in tweet_token_list] # apply word rooter\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "t = []\n",
    "df['clean_tweet'] = df.comment_text.apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be converting the statements to a vector format for the machine to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=0.99, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(df['clean_tweet']).toarray()\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current analysis, lets define the machine to extract 10 unique topics from the dataset (You can play around with the number of topics.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 10\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the machine performs the topic modelling analysis. (This might take a little while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-01c38d5c891a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\sklearn\\decomposition\\_lda.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    579\u001b[0m                     \u001b[1;31m# batch update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                     self._em_step(X, total_samples=n_samples,\n\u001b[1;32m--> 581\u001b[1;33m                                   batch_update=True, parallel=parallel)\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                 \u001b[1;31m# check perplexity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\sklearn\\decomposition\\_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;31m# E-step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         _, suff_stats = self._e_step(X, cal_sstats=True, random_init=True,\n\u001b[1;32m--> 457\u001b[1;33m                                      parallel=parallel)\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;31m# M-step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\sklearn\\decomposition\\_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    408\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_change_tol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                                               random_state)\n\u001b[1;32m--> 410\u001b[1;33m             for idx_slice in gen_even_slices(X.shape[0], n_jobs))\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# merge result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dudas\\anaconda3\\envs\\wintest\\lib\\site-packages\\sklearn\\decomposition\\_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_iters, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             doc_topic_d = (exp_doc_topic_d *\n\u001b[1;32m--> 117\u001b[1;33m                            np.dot(cnts / norm_phi, exp_topic_word_d.T))\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;31m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             _dirichlet_expectation_1d(doc_topic_d, doc_topic_prior,\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to display the topics generated.\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in the table show the most significant words in each topic. With further analysis, we can understand the behaviour of the dataset and type of conversations that occur in them. \n",
    "\n",
    "Try changing the no_top_words variable to show more or less words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "      <th>Topic 6 words</th>\n",
       "      <th>Topic 6 weights</th>\n",
       "      <th>Topic 7 words</th>\n",
       "      <th>Topic 7 weights</th>\n",
       "      <th>Topic 8 words</th>\n",
       "      <th>Topic 8 weights</th>\n",
       "      <th>Topic 9 words</th>\n",
       "      <th>Topic 9 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump</td>\n",
       "      <td>3452.3</td>\n",
       "      <td>mental</td>\n",
       "      <td>3351.9</td>\n",
       "      <td>canada</td>\n",
       "      <td>591.5</td>\n",
       "      <td>mental</td>\n",
       "      <td>1186.5</td>\n",
       "      <td>gun</td>\n",
       "      <td>1385.3</td>\n",
       "      <td>school</td>\n",
       "      <td>840.5</td>\n",
       "      <td>mental</td>\n",
       "      <td>1058.1</td>\n",
       "      <td>white</td>\n",
       "      <td>1220.1</td>\n",
       "      <td>mental</td>\n",
       "      <td>1836.1</td>\n",
       "      <td>god</td>\n",
       "      <td>954.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>presid</td>\n",
       "      <td>1031.5</td>\n",
       "      <td>ill</td>\n",
       "      <td>1993.1</td>\n",
       "      <td>muslim</td>\n",
       "      <td>582.0</td>\n",
       "      <td>peopl</td>\n",
       "      <td>708.3</td>\n",
       "      <td>mental</td>\n",
       "      <td>1156.3</td>\n",
       "      <td>kid</td>\n",
       "      <td>723.0</td>\n",
       "      <td>comment</td>\n",
       "      <td>848.3</td>\n",
       "      <td>peopl</td>\n",
       "      <td>1076.2</td>\n",
       "      <td>peopl</td>\n",
       "      <td>1793.0</td>\n",
       "      <td>one</td>\n",
       "      <td>934.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vote</td>\n",
       "      <td>813.8</td>\n",
       "      <td>health</td>\n",
       "      <td>1213.7</td>\n",
       "      <td>countri</td>\n",
       "      <td>539.3</td>\n",
       "      <td>drug</td>\n",
       "      <td>555.8</td>\n",
       "      <td>peopl</td>\n",
       "      <td>981.1</td>\n",
       "      <td>year</td>\n",
       "      <td>590.5</td>\n",
       "      <td>like</td>\n",
       "      <td>678.6</td>\n",
       "      <td>black</td>\n",
       "      <td>651.0</td>\n",
       "      <td>health</td>\n",
       "      <td>1464.6</td>\n",
       "      <td>women</td>\n",
       "      <td>905.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>780.9</td>\n",
       "      <td>medic</td>\n",
       "      <td>706.8</td>\n",
       "      <td>us</td>\n",
       "      <td>519.8</td>\n",
       "      <td>ill</td>\n",
       "      <td>538.9</td>\n",
       "      <td>law</td>\n",
       "      <td>844.9</td>\n",
       "      <td>go</td>\n",
       "      <td>514.7</td>\n",
       "      <td>would</td>\n",
       "      <td>668.2</td>\n",
       "      <td>disord</td>\n",
       "      <td>537.1</td>\n",
       "      <td>homeless</td>\n",
       "      <td>1367.5</td>\n",
       "      <td>life</td>\n",
       "      <td>830.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elect</td>\n",
       "      <td>579.5</td>\n",
       "      <td>http</td>\n",
       "      <td>630.5</td>\n",
       "      <td>world</td>\n",
       "      <td>490.3</td>\n",
       "      <td>health</td>\n",
       "      <td>497.7</td>\n",
       "      <td>kill</td>\n",
       "      <td>699.6</td>\n",
       "      <td>time</td>\n",
       "      <td>507.9</td>\n",
       "      <td>think</td>\n",
       "      <td>650.4</td>\n",
       "      <td>person</td>\n",
       "      <td>529.5</td>\n",
       "      <td>care</td>\n",
       "      <td>1296.8</td>\n",
       "      <td>peopl</td>\n",
       "      <td>798.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>republican</td>\n",
       "      <td>550.2</td>\n",
       "      <td>help</td>\n",
       "      <td>527.0</td>\n",
       "      <td>canadian</td>\n",
       "      <td>462.9</td>\n",
       "      <td>problem</td>\n",
       "      <td>491.9</td>\n",
       "      <td>polic</td>\n",
       "      <td>683.4</td>\n",
       "      <td>one</td>\n",
       "      <td>500.8</td>\n",
       "      <td>person</td>\n",
       "      <td>629.7</td>\n",
       "      <td>”</td>\n",
       "      <td>486.9</td>\n",
       "      <td>need</td>\n",
       "      <td>1169.6</td>\n",
       "      <td>would</td>\n",
       "      <td>740.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>obama</td>\n",
       "      <td>542.2</td>\n",
       "      <td>need</td>\n",
       "      <td>504.5</td>\n",
       "      <td>islam</td>\n",
       "      <td>448.3</td>\n",
       "      <td>use</td>\n",
       "      <td>476.3</td>\n",
       "      <td>ill</td>\n",
       "      <td>674.7</td>\n",
       "      <td>get</td>\n",
       "      <td>458.6</td>\n",
       "      <td>say</td>\n",
       "      <td>608.7</td>\n",
       "      <td>right</td>\n",
       "      <td>484.1</td>\n",
       "      <td>work</td>\n",
       "      <td>956.9</td>\n",
       "      <td>church</td>\n",
       "      <td>686.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>peopl</td>\n",
       "      <td>540.2</td>\n",
       "      <td>www</td>\n",
       "      <td>467.5</td>\n",
       "      <td>right</td>\n",
       "      <td>409.8</td>\n",
       "      <td>issu</td>\n",
       "      <td>456.4</td>\n",
       "      <td>one</td>\n",
       "      <td>526.8</td>\n",
       "      <td>student</td>\n",
       "      <td>385.0</td>\n",
       "      <td>know</td>\n",
       "      <td>607.6</td>\n",
       "      <td>gender</td>\n",
       "      <td>454.1</td>\n",
       "      <td>ill</td>\n",
       "      <td>903.6</td>\n",
       "      <td>like</td>\n",
       "      <td>656.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>man</td>\n",
       "      <td>499.8</td>\n",
       "      <td>treatment</td>\n",
       "      <td>453.5</td>\n",
       "      <td>like</td>\n",
       "      <td>400.1</td>\n",
       "      <td>caus</td>\n",
       "      <td>444.5</td>\n",
       "      <td>would</td>\n",
       "      <td>521.2</td>\n",
       "      <td>work</td>\n",
       "      <td>373.3</td>\n",
       "      <td>issu</td>\n",
       "      <td>572.7</td>\n",
       "      <td>women</td>\n",
       "      <td>446.7</td>\n",
       "      <td>get</td>\n",
       "      <td>825.9</td>\n",
       "      <td>men</td>\n",
       "      <td>634.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>democrat</td>\n",
       "      <td>488.0</td>\n",
       "      <td>com</td>\n",
       "      <td>415.4</td>\n",
       "      <td>govern</td>\n",
       "      <td>385.2</td>\n",
       "      <td>suicid</td>\n",
       "      <td>405.2</td>\n",
       "      <td>get</td>\n",
       "      <td>519.6</td>\n",
       "      <td>need</td>\n",
       "      <td>370.2</td>\n",
       "      <td>ill</td>\n",
       "      <td>569.4</td>\n",
       "      <td>’</td>\n",
       "      <td>443.7</td>\n",
       "      <td>help</td>\n",
       "      <td>795.8</td>\n",
       "      <td>children</td>\n",
       "      <td>619.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>get</td>\n",
       "      <td>475.6</td>\n",
       "      <td>get</td>\n",
       "      <td>362.2</td>\n",
       "      <td>peopl</td>\n",
       "      <td>378.4</td>\n",
       "      <td>depress</td>\n",
       "      <td>381.5</td>\n",
       "      <td>person</td>\n",
       "      <td>451.4</td>\n",
       "      <td>would</td>\n",
       "      <td>369.8</td>\n",
       "      <td>make</td>\n",
       "      <td>567.4</td>\n",
       "      <td>mental</td>\n",
       "      <td>441.8</td>\n",
       "      <td>money</td>\n",
       "      <td>740.8</td>\n",
       "      <td>know</td>\n",
       "      <td>613.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>white</td>\n",
       "      <td>453.6</td>\n",
       "      <td>doctor</td>\n",
       "      <td>342.3</td>\n",
       "      <td>christian</td>\n",
       "      <td>373.7</td>\n",
       "      <td>gun</td>\n",
       "      <td>369.9</td>\n",
       "      <td>right</td>\n",
       "      <td>422.9</td>\n",
       "      <td>like</td>\n",
       "      <td>340.8</td>\n",
       "      <td>one</td>\n",
       "      <td>553.2</td>\n",
       "      <td>group</td>\n",
       "      <td>415.7</td>\n",
       "      <td>pay</td>\n",
       "      <td>702.1</td>\n",
       "      <td>live</td>\n",
       "      <td>592.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>think</td>\n",
       "      <td>435.9</td>\n",
       "      <td>patient</td>\n",
       "      <td>320.2</td>\n",
       "      <td>liber</td>\n",
       "      <td>358.4</td>\n",
       "      <td>violenc</td>\n",
       "      <td>320.6</td>\n",
       "      <td>go</td>\n",
       "      <td>364.9</td>\n",
       "      <td>cathol</td>\n",
       "      <td>337.7</td>\n",
       "      <td>peopl</td>\n",
       "      <td>513.0</td>\n",
       "      <td>hate</td>\n",
       "      <td>392.3</td>\n",
       "      <td>problem</td>\n",
       "      <td>662.4</td>\n",
       "      <td>child</td>\n",
       "      <td>582.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>support</td>\n",
       "      <td>432.4</td>\n",
       "      <td>year</td>\n",
       "      <td>302.0</td>\n",
       "      <td>one</td>\n",
       "      <td>333.1</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>310.5</td>\n",
       "      <td>could</td>\n",
       "      <td>349.7</td>\n",
       "      <td>educ</td>\n",
       "      <td>329.4</td>\n",
       "      <td>use</td>\n",
       "      <td>494.8</td>\n",
       "      <td>male</td>\n",
       "      <td>363.5</td>\n",
       "      <td>mani</td>\n",
       "      <td>660.3</td>\n",
       "      <td>woman</td>\n",
       "      <td>581.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>donald</td>\n",
       "      <td>410.1</td>\n",
       "      <td>disord</td>\n",
       "      <td>297.7</td>\n",
       "      <td>blind</td>\n",
       "      <td>329.4</td>\n",
       "      <td>addict</td>\n",
       "      <td>278.2</td>\n",
       "      <td>guy</td>\n",
       "      <td>347.4</td>\n",
       "      <td>church</td>\n",
       "      <td>309.2</td>\n",
       "      <td>someon</td>\n",
       "      <td>482.5</td>\n",
       "      <td>one</td>\n",
       "      <td>360.8</td>\n",
       "      <td>would</td>\n",
       "      <td>660.1</td>\n",
       "      <td>love</td>\n",
       "      <td>553.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 0 words Topic 0 weights Topic 1 words Topic 1 weights Topic 2 words  \\\n",
       "0          trump          3452.3        mental          3351.9        canada   \n",
       "1         presid          1031.5           ill          1993.1        muslim   \n",
       "2           vote           813.8        health          1213.7       countri   \n",
       "3           like           780.9         medic           706.8            us   \n",
       "4          elect           579.5          http           630.5         world   \n",
       "5     republican           550.2          help           527.0      canadian   \n",
       "6          obama           542.2          need           504.5         islam   \n",
       "7          peopl           540.2           www           467.5         right   \n",
       "8            man           499.8     treatment           453.5          like   \n",
       "9       democrat           488.0           com           415.4        govern   \n",
       "10           get           475.6           get           362.2         peopl   \n",
       "11         white           453.6        doctor           342.3     christian   \n",
       "12         think           435.9       patient           320.2         liber   \n",
       "13       support           432.4          year           302.0           one   \n",
       "14        donald           410.1        disord           297.7         blind   \n",
       "\n",
       "   Topic 2 weights Topic 3 words Topic 3 weights Topic 4 words  \\\n",
       "0            591.5        mental          1186.5           gun   \n",
       "1            582.0         peopl           708.3        mental   \n",
       "2            539.3          drug           555.8         peopl   \n",
       "3            519.8           ill           538.9           law   \n",
       "4            490.3        health           497.7          kill   \n",
       "5            462.9       problem           491.9         polic   \n",
       "6            448.3           use           476.3           ill   \n",
       "7            409.8          issu           456.4           one   \n",
       "8            400.1          caus           444.5         would   \n",
       "9            385.2        suicid           405.2           get   \n",
       "10           378.4       depress           381.5        person   \n",
       "11           373.7           gun           369.9         right   \n",
       "12           358.4       violenc           320.6            go   \n",
       "13           333.1       alcohol           310.5         could   \n",
       "14           329.4        addict           278.2           guy   \n",
       "\n",
       "   Topic 4 weights Topic 5 words Topic 5 weights Topic 6 words  \\\n",
       "0           1385.3        school           840.5        mental   \n",
       "1           1156.3           kid           723.0       comment   \n",
       "2            981.1          year           590.5          like   \n",
       "3            844.9            go           514.7         would   \n",
       "4            699.6          time           507.9         think   \n",
       "5            683.4           one           500.8        person   \n",
       "6            674.7           get           458.6           say   \n",
       "7            526.8       student           385.0          know   \n",
       "8            521.2          work           373.3          issu   \n",
       "9            519.6          need           370.2           ill   \n",
       "10           451.4         would           369.8          make   \n",
       "11           422.9          like           340.8           one   \n",
       "12           364.9        cathol           337.7         peopl   \n",
       "13           349.7          educ           329.4           use   \n",
       "14           347.4        church           309.2        someon   \n",
       "\n",
       "   Topic 6 weights Topic 7 words Topic 7 weights Topic 8 words  \\\n",
       "0           1058.1         white          1220.1        mental   \n",
       "1            848.3         peopl          1076.2         peopl   \n",
       "2            678.6         black           651.0        health   \n",
       "3            668.2        disord           537.1      homeless   \n",
       "4            650.4        person           529.5          care   \n",
       "5            629.7             ”           486.9          need   \n",
       "6            608.7         right           484.1          work   \n",
       "7            607.6        gender           454.1           ill   \n",
       "8            572.7         women           446.7           get   \n",
       "9            569.4             ’           443.7          help   \n",
       "10           567.4        mental           441.8         money   \n",
       "11           553.2         group           415.7           pay   \n",
       "12           513.0          hate           392.3       problem   \n",
       "13           494.8          male           363.5          mani   \n",
       "14           482.5           one           360.8         would   \n",
       "\n",
       "   Topic 8 weights Topic 9 words Topic 9 weights  \n",
       "0           1836.1           god           954.9  \n",
       "1           1793.0           one           934.0  \n",
       "2           1464.6         women           905.2  \n",
       "3           1367.5          life           830.1  \n",
       "4           1296.8         peopl           798.2  \n",
       "5           1169.6         would           740.9  \n",
       "6            956.9        church           686.1  \n",
       "7            903.6          like           656.3  \n",
       "8            825.9           men           634.9  \n",
       "9            795.8      children           619.3  \n",
       "10           740.8          know           613.1  \n",
       "11           702.1          live           592.2  \n",
       "12           662.4         child           582.4  \n",
       "13           660.3         woman           581.8  \n",
       "14           660.1          love           553.7  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go through the topics achieved to understand the most common conversations that occur in this dataset. We will be looking into this content in detail in the following section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now to the analysis!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}