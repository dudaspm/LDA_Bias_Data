{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e68dd8f-aab6-4969-be1c-85e3958eb5ca",
   "metadata": {},
   "source": [
    "# Topic Modeling - üêì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7c30d-baa6-4186-a98a-f090f82e38a2",
   "metadata": {},
   "source": [
    "![Alice's Adventures in Wonderland](https://www.gutenberg.org/files/11/11-h/images/cover.jpg)\n",
    "\n",
    "> Alice's Adventures in Wonderland {cite:p}`Carroll` via https://www.gutenberg.org/files/11/11-h/11-h.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edb20a-f69d-4bce-8c40-035f1965df70",
   "metadata": {},
   "source": [
    "To start our discussion, we should introduce what Topic Modeling is and how it can be applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172a2e2-8392-4287-805a-388c71209bd8",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\"Topic modeling is a princpled approach for discovering topics from a large corpus of text documents {cite:p}`liu2020sentiment` (pg.159).\"\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf84bb5-fbd4-49a2-a75f-a626feb9bb38",
   "metadata": {},
   "source": [
    "Already, we have few things to unpack. What are topics? How are the defined? Do we define or does the computer? What is a large corpus? How many documents do we need?\n",
    "\n",
    "\n",
    "Let's start with a *large corpus of text documents*. Typically, we would have two documents üìÑ, five documents üìÑ, ten million documents üìÑ, can be thought of as our corpus. Yes, even 1 document üìÑ can be used for topic modeling. So, defining, *large corpus of text documents*, can be subjective. \n",
    "\n",
    "As specified by Liu {cite:p}`liu2020sentiment` we can start this conversation using one of the two basic types of topic modeling. This being *probablistic Latent Dirichlet Allocation* or *Latent Dirichlet Allocation*. For our conversation, we will be using *Latent Dirichlet Allocation*. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d41a2-a582-4123-8601-066793c98966",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "### Pronunciation\n",
    "\n",
    "#### Latent\n",
    "<audio controls>\n",
    "  <source src=\"https://github.com/dudaspm/LDA_Bias_Data/blob/main/audio/Latent.mp3?raw=true\"\n",
    "          type=\"audio/mp3\">\n",
    "Your browser does not support the audio element.\n",
    "</audio>\n",
    "\n",
    "#### Dirichlet\n",
    "<audio controls>\n",
    "  <source src=\"https://github.com/dudaspm/LDA_Bias_Data/blob/main/audio/Dirichlet.mp3?raw=true\"\n",
    "          type=\"audio/mp3\">\n",
    "Your browser does not support the audio element.\n",
    "</audio>\n",
    "\n",
    "#### Allocation\n",
    "<audio controls>\n",
    "  <source src=\"https://github.com/dudaspm/LDA_Bias_Data/blob/main/audio/Allocation.mp3?raw=true\"\n",
    "          type=\"audio/mp3\">\n",
    "Your browser does not support the audio element.\n",
    "</audio>\n",
    "\n",
    "Our pronoucation stems from a talk by David Blei who is a professor of Statistics and Computer Science at Columbia University during David's talk \"Probabilistic Topic Models and User Behavior {cite:p}`Blei_2017`.\" The citation provides a link to original YouTube video (which is a *great* resource), but specifically, helpful for the pronouciation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e206b8d-bced-4d35-a346-8e1c45441936",
   "metadata": {},
   "source": [
    "## What is Latent Dirichlet Allocation or LDA?\n",
    "\n",
    "LDA is an unsupervised learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde1589-0694-4185-a22f-af82265f034b",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Topic Modeling with Documents  üìÑ           \n",
    "            \n",
    "* supervised - Our documents üìÑ are pre-labeled with the given topic(s). We can then train üèãÔ∏è and test üß™ (and also, you can include validate). **Usually** this is split:\n",
    "    * training üèãÔ∏è 80% \n",
    "    * testing üß™ 20%. \n",
    "            \n",
    "   \n",
    "* unsupervised - Data is not labeled. So, we have no idea what the topics are beforehand. That being said, we can (and will) define the *number of topics*. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bab6c7-8078-483b-a84a-dec916201024",
   "metadata": {},
   "source": [
    "So, coming back to our original questions:\n",
    "* What are topics? \n",
    "    * The topics will X number of sets of terms (we define this X) which will (could) have a common theme. \n",
    "* How are they defined? \n",
    "    * This is what we will get to in this notebook.     \n",
    "* Do we define or does the computer? \n",
    "    * LDA is unsupervised, so we define the number of topics. The computer provides the topics themselves. \n",
    "* What is a large corpus? and How many documents do we need? \n",
    "    * A bit subjective here. There is a *great* discussion about this by Tang et al.  {cite:p}`tang2014understanding` regarding this. If you have chance, read all the points, but to sum it up\n",
    "        * The number of documents does matter, but at some point, increasing the number does not warrent better results. Even sampling a 1000 papers from 1000000 papers could result in the same, if not, better results than using 1000000 documents. \n",
    "        * The size of the documents also plays a role, so documents should not be short. Larger documents can be sampled and again, receive the same desired output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbe6f0-e011-4ca9-b564-de59f28159b0",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "043de121-b7f6-4ec5-bed6-88f81c577e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/11/11-h/11-h.htm\"\n",
    "html = urlopen(url).read()\n",
    "soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "text = soup.get_text()\n",
    "print (type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7db0482-3d3e-4530-ac72-2a4fdb658c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27305\n",
      "['sadf', 'Aewrwe']\n",
      "that\n",
      "you never tasted an egg!‚Äù\n",
      "\n",
      "\n",
      "‚Äúi have tasted eggs, certainly,‚Äù said alice, who was a very\n",
      "tru\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "textArray = text.lower().split(\" \")\n",
    "print(len(textArray))\n",
    "documents = []\n",
    "\n",
    "print (random.choices([\"sadf\",\"asdf\",\"Aewrwe\"], k = 2))\n",
    "for i in range(27):\n",
    "    documents.append(\" \".join(textArray[i*1000:(i+1)*1000]))\n",
    "\n",
    "print(documents[10][0:100])\n",
    "cv = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "df = cv.fit_transform(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f00d8bf9-c910-49a7-b01b-f1fa3193f41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5, random_state = 42)\n",
    "lda.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8e1c93a-08ab-4b37-9ccb-74941b7f9e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words for Topic #0\n",
      "['at', 'her', 'as', 'that', 'was', 'in', 'you', 'alice', 'said', 'of', 'she', 'it', 'to', 'and', 'the']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #1\n",
      "['by', 'works', 'any', 'with', 'work', 'this', 'in', 'and', 'you', 'to', 'project', 'or', 'gutenberg', 'of', 'the']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #2\n",
      "['jogged', 'leading', 'blame', 'chanced', 'doze', 'unwillingly', 'interrupt', 'brush', 'offend', 'roots', 'civil', 'personal', 'remarks', 'splashing', 'humbly']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #3\n",
      "['jogged', 'leading', 'blame', 'chanced', 'doze', 'unwillingly', 'interrupt', 'brush', 'offend', 'roots', 'civil', 'personal', 'remarks', 'splashing', 'humbly']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #4\n",
      "['be', 'that', 'her', 'as', 'in', 'was', 'alice', 'of', 'said', 'you', 'she', 'it', 'to', 'and', 'the']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271035d7-1400-43e3-97db-6aae31ea1c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
